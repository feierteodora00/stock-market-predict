{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Initial_DJIA_Pred_using_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSQ-bNXfU5Ux"
      },
      "source": [
        "# Stock market predictions based on news\n",
        "The prediction is made based on different news on that days and correlated with the respective price movements (1-up or 0-down). It works as a binary classifier. For this program to work, the DJIA.csv file must be uploaded before starting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nut2uAZ4OZGY"
      },
      "source": [
        "#install libraries\n",
        "!pip install keras\n",
        "!pip install nltk\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USuFkQwt3jYU"
      },
      "source": [
        "#import and download required libraries\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3MWW3z63-V-"
      },
      "source": [
        "#read the data\n",
        "dataframe = pd.read_csv('DJIA.csv')\n",
        "dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lay159X97kth"
      },
      "source": [
        "#split data into features (headlines/corpus) and labels (stock movement classified as up and down)\n",
        "dataframe['Headlines'] = dataframe[dataframe.columns[2:]].apply(lambda x: '. '.join(x.dropna().astype(str)),axis=1) #dropna used for the last columns which have nan values\n",
        "corpus = dataframe['Headlines']\n",
        "labels = dataframe['Label']\n",
        "#remove punctuation and make lowercase\n",
        "corpus.replace(\"[^a-zA-Z]\", \" \", regex=True, inplace=True) \n",
        "corpus = corpus.str.lower()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNCrVzl3Ahw-"
      },
      "source": [
        "#calculate the sentiment value for each line (all news of a day)\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "results = [] #will contain the compound score\n",
        "for line in corpus:\n",
        "  pol_score = sia.polarity_scores(line)\n",
        "  results.append(pol_score)\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orYtt-E6Al3O"
      },
      "source": [
        "#move the results to a dataframe and see the correlation\n",
        "score = pd.DataFrame(results)['compound']\n",
        "score.corr(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLu5UB6ZBNHk"
      },
      "source": [
        "The correlation value will be around 0, meaning that there is no correlation between the sentiment of the news and the stock movement. So a resonable approach is to create a model to work out the movement directly from text rather than sentiment score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2aezV9h7vpD"
      },
      "source": [
        "#convert the datatypes so it can be worked with them later\n",
        "corpus =  corpus.tolist()\n",
        "labels = labels.to_numpy()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avdGKpzrxAJz"
      },
      "source": [
        "#set the stop words (including the leading 'b' which appears in some sentences)\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "stop_words.add(\"b\") #the letter 'b' appear at the beginning of some headlines and it is redundant"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj7Y2jASxzbx"
      },
      "source": [
        "#remove stop words \n",
        "corpus_without_sw = []\n",
        "#for every sentence\n",
        "for sent in corpus:\n",
        "  sent_tokens = word_tokenize(sent)\n",
        "  tokens_without_sw = [w for w in sent_tokens if not w in stop_words]\n",
        "  filtered_sent = (\" \").join(tokens_without_sw)\n",
        "  #print(filtered_sent)\n",
        "  corpus_without_sw.append(filtered_sent)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8nASjNICqbo"
      },
      "source": [
        "#tokenize the corpus \n",
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts(corpus_without_sw)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQLqstqRC6f0"
      },
      "source": [
        "#get the vocabulary length which is used in the first layer of the model\n",
        "vocab_length = len(word_tokenizer.word_index) +1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi0pJgDRDCCh"
      },
      "source": [
        "#convert all the sentences (lines) in corpus to numeric arrays \n",
        "embedded_sentences = word_tokenizer.texts_to_sequences(corpus_without_sw)\n",
        "#print(embedded_sentences)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5f0KaIyDHFs"
      },
      "source": [
        "#get size of the largest line (in number of words) and pad the other lines with zeros at the end until they reach that size\n",
        "word_count = lambda sentence: len(word_tokenize(sentence))\n",
        "longest_sentence = max(corpus_without_sw, key=word_count)\n",
        "length_long_sentence = len(word_tokenize(longest_sentence))\n",
        "\n",
        "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
        "#print(padded_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nglRd7RY1XhY"
      },
      "source": [
        "For testing, the headlines which will be used must be formated to have the length of length_long_sentence for the neural network to work. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnVgsSiQDdVg"
      },
      "source": [
        "#split the data into training and testing\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_sentences, labels, test_size=0.3)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k_7YEVAEmmZ"
      },
      "source": [
        "#create a model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_length, 20, input_length=length_long_sentence))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ6jcXFYE6e5"
      },
      "source": [
        "#fit training data to the model\n",
        "model.fit(x_train, y_train, epochs=50, verbose =1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViqtaVIwFCDU"
      },
      "source": [
        "#calculate loss and accuracy \n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(accuracy*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLtA_b26FicV"
      },
      "source": [
        "#create a confusion matrix for the prediction results\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred = model.predict_classes(x_test)\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}